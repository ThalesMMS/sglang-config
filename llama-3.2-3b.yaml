# SGLang Configuration for Llama 3.2 3B Instruct
# Usage: python -m sglang.launch_server --config llama-3.2-3b.yaml

# Model configuration
model-path: unsloth/Llama-3.2-3B-Instruct

# Network settings
host: 0.0.0.0
port: 8000

# Context and memory settings
context-length: 16384         # Maximum context window (128K native, limited for memory)
max-running-requests: 16      # Maximum concurrent requests
mem-fraction-static: 0.90     # GPU memory utilization (90%)

# Performance settings
dtype: half                   # Use FP16 for efficiency
attention-backend: flashinfer # Use FlashInfer for attention

# Optional: Enable metrics and logging
# enable-metrics: true
# log-requests: true
# log-level: info
